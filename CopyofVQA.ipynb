{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "id": "msGVhonI-od1",
    "outputId": "90513778-85c5-46c4-a502-7e64537ce468"
   },
   "outputs": [],
   "source": [
    " Upgrade pillow to latest version (solves a colab Issue) :\n",
    "!pip install --user Pillow>=5.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PrAaNtBw-od3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcxyz\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "print(\"abcxyz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-F8cMOPD-od4"
   },
   "source": [
    "### Download the Prebuilt VQA model and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yvrO8B6y-od6",
    "outputId": "3dfb4eb1-a9a9-42c5-a474-32c700939657",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/Cyanogenoid/pytorch-vqa/releases!pip install wget\n",
    "import wget\n",
    "url = 'https://github.com/Cyanogenoid/pytorch-vqa/releases/download/v1.0/2017-08-04_00.55.19.pth'\n",
    "if not os.path.isfile('./2017-08-04_00.55.19.pth'):   # 81Mb model\n",
    "    #!wget https://github.com/Cyanogenoid/pytorch-vqa/releases/download/v1.0/2017-08-04_00.55.19.pth\n",
    "    wget.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fUYCtjid-od7"
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    import torch\n",
    "except:\n",
    "    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "    accelerator = 'cu80' if os.path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
    "    !pip install -q \\\n",
    "      http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl \\\n",
    "      torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BIDGkC5w-od8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14euJtBq-od9",
    "outputId": "44c98d82-7b84-4920-978d-1844c89493e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'tracker', 'config', 'weights', 'eval', 'vocab'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import model # from pytorch-vqa\n",
    "\n",
    "#saved_state = torch.load('logs/2017-08-04_00:55:19.pth')\n",
    "saved_state = torch.load('./2017-08-04_00.55.19.pth', map_location=device)\n",
    "tokens = len(saved_state['vocab']['question']) + 1\n",
    "\n",
    "saved_state.keys()  # See what's in the saved state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KwNuREsw-od-",
    "outputId": "4047f0c0-4d61-469d-ced5-47f097f34496",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Net(\n",
       "    (text): TextProcessor(\n",
       "      (embedding): Embedding(15193, 300, padding_idx=0)\n",
       "      (drop): Dropout(p=0.5, inplace=False)\n",
       "      (tanh): Tanh()\n",
       "      (lstm): LSTM(300, 1024)\n",
       "    )\n",
       "    (attention): Attention(\n",
       "      (v_conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (q_lin): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (x_conv): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (drop): Dropout(p=0.5, inplace=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (classifier): Classifier(\n",
       "      (drop1): Dropout(p=0.5, inplace=False)\n",
       "      (lin1): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "      (relu): ReLU()\n",
       "      (drop2): Dropout(p=0.5, inplace=False)\n",
       "      (lin2): Linear(in_features=1024, out_features=3000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the predefined model\n",
    "vqa_net = torch.nn.DataParallel(model.Net(tokens))\n",
    "vqa_net.load_state_dict(saved_state['weights'])\n",
    "vqa_net.to(device)\n",
    "vqa_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWjjR0II-od_"
   },
   "source": [
    "### Now get the Correct Image feature network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103,
     "referenced_widgets": [
      "87ca56ccc6c3444ba310ac1195074d3e",
      "8ed2ea24126b42a989c7d67be8543668",
      "8e750c9d72b044aa86fc395f3f24dc92",
      "e9fe826b5bba48b18a99a5bd67b7381b",
      "3618a5c9a96e4317acae6bf1b7c52975",
      "2e48e66d514243db9623f6d29cf19e04",
      "a0c6e1b729e74b018fbb021cc541aa0e",
      "252a94fb074a4f0798664e217d7dda54"
     ]
    },
    "id": "1eB1tx19-oeA",
    "outputId": "ede70702-6f4c-47d5-d0ed-b398520ee38e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import resnet  # from pytorch-resnet\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "def get_transform(target_size, central_fraction=1.0):\n",
    "    return transforms.Compose([\n",
    "        transforms.Scale(int(target_size / central_fraction)),\n",
    "        transforms.CenterCrop(target_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "class ResNetLayer4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetLayer4, self).__init__()\n",
    "        self.model = resnet.resnet152(pretrained=True)\n",
    "        \n",
    "        # from  visual_qa_analysis/config.py\n",
    "        image_size = 448  # scale shorter end of image to this size and centre crop\n",
    "        #output_size = image_size // 32  # size of the feature maps after processing through a network\n",
    "        output_features = 2048  # number of feature maps thereof\n",
    "        central_fraction = 0.875 # only take this much of the centre when scaling and centre cropping\n",
    "\n",
    "        self.transform = get_transform(image_size, central_fraction)\n",
    "\n",
    "        def save_output(module, input, output):\n",
    "            self.buffer = output\n",
    "        self.model.layer4.register_forward_hook(save_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.model(x)\n",
    "        return self.buffer\n",
    "    \n",
    "    def image_to_features(self, img_file):\n",
    "        img = Image.open(img_file).convert('RGB')\n",
    "        img_transformed = self.transform(img)\n",
    "        #print(img_transformed.size())\n",
    "        img_batch = img_transformed.unsqueeze(0).to(device)\n",
    "        print(img_batch)\n",
    "        return self.forward(img_batch) \n",
    "    \n",
    "resnet_layer4 = ResNetLayer4().to(device)  # Downloads 241Mb model when first run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c99N7gK5-oeC",
    "outputId": "c029b44b-992c-4e03-feaa-16be6cc7be68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./img/Black_pussy_-_panoramio.jpg',\n",
       " './img/jump.jpg',\n",
       " './img/abc.jpg',\n",
       " './img/penguins.jpg',\n",
       " './img/2.jpg',\n",
       " './img/ima.jpg',\n",
       " './img/cat_roof_home_architecture_building_roofs_animal_sit-536976.jpg!d']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample images : \n",
    "import wget\n",
    "## local images----------\n",
    "image_files=['./img/Black_pussy_-_panoramio.jpg','./img/jump.jpg','./img/abc.jpg',\n",
    "    './img/penguins.jpg',\n",
    "    './img/2.jpg',\n",
    "             './img/ima.jpg',\n",
    " './img/cat_roof_home_architecture_building_roofs_animal_sit-536976.jpg!d']\n",
    "image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "ZBsh9Fml-oeD",
    "outputId": "2c69b22f-3697-4898-8f46-1d512d76753c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.6049,  0.6049,  0.6049,  ..., -0.2856, -0.2856, -0.2856],\n",
      "          [ 0.6221,  0.6049,  0.6049,  ..., -0.2856, -0.2856, -0.2856],\n",
      "          [ 0.6221,  0.6221,  0.6049,  ..., -0.2684, -0.2684, -0.2684],\n",
      "          ...,\n",
      "          [-0.9705, -1.0390, -1.1589,  ...,  2.1975,  2.1975,  2.1975],\n",
      "          [-1.2617, -1.2788, -1.2617,  ...,  2.1975,  2.1975,  2.1975],\n",
      "          [-1.3302, -1.3473, -1.2959,  ...,  2.1975,  2.1975,  2.1975]],\n",
      "\n",
      "         [[ 1.2906,  1.2906,  1.2906,  ...,  0.6604,  0.6604,  0.6604],\n",
      "          [ 1.3081,  1.2906,  1.2906,  ...,  0.6604,  0.6604,  0.6604],\n",
      "          [ 1.3081,  1.3081,  1.2906,  ...,  0.6779,  0.6779,  0.6779],\n",
      "          ...,\n",
      "          [-0.8627, -0.9328, -1.0553,  ...,  2.3585,  2.3585,  2.3585],\n",
      "          [-1.1604, -1.1779, -1.1604,  ...,  2.3585,  2.3585,  2.3585],\n",
      "          [-1.1954, -1.2129, -1.1779,  ...,  2.3585,  2.3585,  2.3585]],\n",
      "\n",
      "         [[ 2.2740,  2.2740,  2.2740,  ...,  2.0648,  2.0648,  2.0648],\n",
      "          [ 2.2914,  2.2740,  2.2740,  ...,  2.0648,  2.0648,  2.0648],\n",
      "          [ 2.2914,  2.2914,  2.2740,  ...,  2.0648,  2.0648,  2.0648],\n",
      "          ...,\n",
      "          [-1.0898, -1.1596, -1.2816,  ...,  2.5006,  2.5006,  2.5006],\n",
      "          [-1.3861, -1.4036, -1.3861,  ...,  2.5006,  2.5006,  2.5006],\n",
      "          [-1.4210, -1.4559, -1.3861,  ...,  2.5006,  2.5006,  2.5006]]]])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "v = resnet_layer4.image_to_features(image_files[1])\n",
    "v.size()\n",
    "print(type(v))\n",
    "\n",
    "#for j in v:\n",
    "    #print(j)\n",
    "    #np.savez('0.npz', x=feats[j].reshape(1024, 196).transpose(1,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRCiyBRL-oeE"
   },
   "source": [
    "### Have a look at how the vocab is built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "S38sg9DH-oeE",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1), ('is', 2), ('what', 3), ('are', 4), ('this', 5)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = saved_state['vocab']\n",
    "vocab.keys()  # dict_keys(['question', 'answer'])\n",
    "list(vocab['question'].items())[:5]  # [('the', 1), ('is', 2), ('what', 3), ('are', 4), ('this', 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Fxoo7u-T-oeG"
   },
   "outputs": [],
   "source": [
    "qtoken_to_index = vocab['question']\n",
    "QUESTION_LENGTH_MAX = 30 # say...\n",
    "    \n",
    "def encode_question(question_str):\n",
    "    \"\"\" Turn a question into a vector of indices and a question length \"\"\"\n",
    "    question_arr = question_str.lower().split(' ')\n",
    "    vec = torch.zeros(len(question_arr)).long()  \n",
    "    for i, token in enumerate(question_arr):\n",
    "        vec[i] = qtoken_to_index.get(token, 0)\n",
    "    return vec.to(device), torch.tensor( len(question_arr) ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "o_Gg5sXq-oeH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yes', 0), ('no', 1), ('2', 2), ('1', 3), ('white', 4)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab['answer'].items())[:5]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "69zsTkpb-oeI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, ['yes', 'no', '2', '1', 'white', '3', 'red', 'blue', '4', 'green'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_words = ['UNDEF'] * len(vocab['answer'])\n",
    "for w,idx in vocab['answer'].items():\n",
    "    answer_words[idx]=w\n",
    "len(answer_words), answer_words[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "vRyZbe43-oeJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True, True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Important things to know...\n",
    "'colour' in qtoken_to_index, 'color' in qtoken_to_index, 'tabby' in answer_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "1rSygLwq-oeK"
   },
   "outputs": [],
   "source": [
    "image_idx = 1\n",
    "image_filename = image_files[image_idx]\n",
    "\n",
    "#img = Image.open(image_filename).convert('RGB')\n",
    "#plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "ou722MJr-oeL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.6049,  0.6049,  0.6049,  ..., -0.2856, -0.2856, -0.2856],\n",
      "          [ 0.6221,  0.6049,  0.6049,  ..., -0.2856, -0.2856, -0.2856],\n",
      "          [ 0.6221,  0.6221,  0.6049,  ..., -0.2684, -0.2684, -0.2684],\n",
      "          ...,\n",
      "          [-0.9705, -1.0390, -1.1589,  ...,  2.1975,  2.1975,  2.1975],\n",
      "          [-1.2617, -1.2788, -1.2617,  ...,  2.1975,  2.1975,  2.1975],\n",
      "          [-1.3302, -1.3473, -1.2959,  ...,  2.1975,  2.1975,  2.1975]],\n",
      "\n",
      "         [[ 1.2906,  1.2906,  1.2906,  ...,  0.6604,  0.6604,  0.6604],\n",
      "          [ 1.3081,  1.2906,  1.2906,  ...,  0.6604,  0.6604,  0.6604],\n",
      "          [ 1.3081,  1.3081,  1.2906,  ...,  0.6779,  0.6779,  0.6779],\n",
      "          ...,\n",
      "          [-0.8627, -0.9328, -1.0553,  ...,  2.3585,  2.3585,  2.3585],\n",
      "          [-1.1604, -1.1779, -1.1604,  ...,  2.3585,  2.3585,  2.3585],\n",
      "          [-1.1954, -1.2129, -1.1779,  ...,  2.3585,  2.3585,  2.3585]],\n",
      "\n",
      "         [[ 2.2740,  2.2740,  2.2740,  ...,  2.0648,  2.0648,  2.0648],\n",
      "          [ 2.2914,  2.2740,  2.2740,  ...,  2.0648,  2.0648,  2.0648],\n",
      "          [ 2.2914,  2.2914,  2.2740,  ...,  2.0648,  2.0648,  2.0648],\n",
      "          ...,\n",
      "          [-1.0898, -1.1596, -1.2816,  ...,  2.5006,  2.5006,  2.5006],\n",
      "          [-1.3861, -1.4036, -1.3861,  ...,  2.5006,  2.5006,  2.5006],\n",
      "          [-1.4210, -1.4559, -1.3861,  ...,  2.5006,  2.5006,  2.5006]]]])\n"
     ]
    }
   ],
   "source": [
    "v0 = resnet_layer4.image_to_features(image_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "e8T_SBAu-oeL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  2,   1,  43, 576, 109,  25, 168]), tensor(7))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q, q_len = encode_question(\"is the cat jumping up or down\")\n",
    "q, q_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "W3NGKhjw-oeL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-12.7777, -12.9656, -18.9573,  ..., -49.5790, -41.9412, -56.8924]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = vqa_net(v0, q.unsqueeze(0), q_len.unsqueeze(0))\n",
    "ans.data.cpu()[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "MbUnvYP8-oeM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([96])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'down'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, answer_idx = ans.data.cpu().max(dim=1)\n",
    "print(answer_idx)\n",
    "answer_words[answer_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "PgzG4RQ_-oeN"
   },
   "outputs": [],
   "source": [
    "\n",
    "def vqa_single_softmax(im_features, q_str):\n",
    "    q, q_len = encode_question(q_str)\n",
    "    ans = vqa_net(im_features, q.unsqueeze(0), q_len.unsqueeze(0))\n",
    "    return ans.data.cpu()\n",
    "\n",
    "def vqa(image_filename, question_arr):\n",
    "    plt.imshow(Image.open(image_filename).convert('RGB')); plt.show()    \n",
    "    image_features = resnet_layer4.image_to_features(image_filename)\n",
    "    for question_str in question_arr:\n",
    "        _, answer_idx = vqa_single_softmax(image_features, question_str).max(dim=1)\n",
    "        print(question_str+\" -> \"+answer_words[ answer_idx ])\n",
    "        print((answer_words[ answer_idx ]+' '*8)[:8]+\" <- \"+question_str)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "3o4UXYLj-oeO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.5699, -1.5357, -1.5357,  ..., -1.0390, -1.3302, -1.3302],\n",
      "          [-1.6213, -1.5870, -1.5870,  ..., -1.1075, -1.2959, -1.2617],\n",
      "          [-1.6898, -1.6384, -1.6384,  ..., -1.1760, -1.2788, -1.2445],\n",
      "          ...,\n",
      "          [ 0.7591,  0.7591,  0.8276,  ...,  0.4166, -0.2513, -1.2103],\n",
      "          [ 0.7419,  0.7077,  0.7077,  ...,  0.4851,  0.2967, -0.3198],\n",
      "          [ 0.7591,  0.7762,  0.7762,  ...,  0.3481,  0.1939,  0.0741]],\n",
      "\n",
      "         [[-1.4055, -1.3704, -1.3704,  ..., -0.8452, -1.1429, -1.1429],\n",
      "          [-1.4405, -1.4055, -1.4055,  ..., -0.9153, -1.1078, -1.0728],\n",
      "          [-1.4405, -1.3880, -1.3880,  ..., -0.9853, -1.0903, -1.0553],\n",
      "          ...,\n",
      "          [ 1.1155,  1.1155,  1.1856,  ...,  0.7654,  0.0651, -0.9328],\n",
      "          [ 1.0980,  1.0630,  1.0630,  ...,  0.8179,  0.6254, -0.0049],\n",
      "          [ 1.1155,  1.1331,  1.1331,  ...,  0.7304,  0.5553,  0.4328]],\n",
      "\n",
      "         [[-1.1596, -1.1247, -1.1247,  ..., -0.7238, -1.0027, -0.9853],\n",
      "          [-1.2119, -1.1770, -1.1770,  ..., -0.7936, -0.9678, -0.9156],\n",
      "          [-1.2293, -1.1770, -1.1770,  ..., -0.8633, -0.9504, -0.8981],\n",
      "          ...,\n",
      "          [ 1.5420,  1.5420,  1.6117,  ...,  0.9842,  0.3045, -0.6890],\n",
      "          [ 1.5245,  1.4897,  1.4897,  ...,  1.0888,  0.8971,  0.2696],\n",
      "          [ 1.5420,  1.5594,  1.5768,  ...,  1.0539,  0.8971,  0.7576]]]])\n",
      "yes      <- is there a cat in the picture\n",
      "no       <- is this a picture of a panda\n",
      "dog      <- is the animal in the picture a panda or a dog\n",
      "black    <- what color is the panda\n",
      "2        <- how many cows are there\n"
     ]
    }
   ],
   "source": [
    "image_idx = 0  # 6 \n",
    "image_files=['./img/penguins.jpg',\n",
    " './img/2.jpg',\n",
    " './img/ima.jpg',\n",
    " './img/cat_roof_home_architecture_building_roofs_animal_sit-536976.jpg!d']\n",
    "vqa(image_files[image_idx], [\n",
    "    \"is there a cat in the picture\",\n",
    "    \"is this a picture of a panda\",\n",
    "    \"is the animal in the picture a panda or a dog\",\n",
    "    \"what color is the panda\",\n",
    "    \"how many cows are there\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynjoYe8G-oed"
   },
   "source": [
    "#### Leave one word out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "UUyN9soR-oee"
   },
   "outputs": [],
   "source": [
    "def leave_one_out(image_filename, question_base):\n",
    "    plt.imshow(Image.open(image_filename).convert('RGB')); plt.show()    \n",
    "    image_features = resnet_layer4.image_to_features(image_filename)\n",
    "    question_arr = question_base.lower().split(' ')\n",
    "    for i, word_omit in enumerate(question_arr):\n",
    "        question_str = ' '.join( question_arr[:i]+question_arr[i+1:] )\n",
    "        score, answer_idx = vqa_single_softmax(image_features, question_str).max(dim=1)\n",
    "        print(question_str+\" -> \"+answer_words[ answer_idx ])\n",
    "        print((answer_words[ answer_idx ]+' '*8)[:8]+\" <- \"+question_str)  #, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "IdxYdNEz-oef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-2.0323, -2.0152, -2.0152,  ..., -1.6213, -1.6555, -1.7754],\n",
      "          [-2.0323, -2.0665, -2.0494,  ..., -1.6384, -1.6727, -1.7925],\n",
      "          [-1.6555, -1.6384, -1.8268,  ..., -1.6555, -1.6898, -1.7412],\n",
      "          ...,\n",
      "          [-1.9980, -1.9638, -1.9638,  ...,  0.7591,  1.1015,  1.1187],\n",
      "          [-1.9980, -1.9638, -1.9980,  ...,  0.8104,  0.7248,  0.6563],\n",
      "          [-2.0152, -1.9980, -1.9980,  ...,  0.8789,  0.5707,  0.4679]],\n",
      "\n",
      "         [[-1.9657, -1.9307, -1.9307,  ..., -1.5805, -1.6155, -1.7206],\n",
      "          [-1.9657, -1.9832, -1.9657,  ..., -1.5980, -1.6331, -1.7206],\n",
      "          [-1.5805, -1.5455, -1.7381,  ..., -1.5630, -1.5980, -1.6331],\n",
      "          ...,\n",
      "          [-1.9307, -1.9307, -1.9307,  ...,  0.6429,  1.0280,  1.0105],\n",
      "          [-1.9307, -1.9482, -1.9657,  ...,  0.6254,  0.4853,  0.6078],\n",
      "          [-1.9132, -1.9482, -1.9832,  ...,  0.5378,  0.1877,  0.3803]],\n",
      "\n",
      "         [[-1.7696, -1.6999, -1.6999,  ..., -1.4907, -1.5256, -1.5953],\n",
      "          [-1.7522, -1.7522, -1.7347,  ..., -1.4907, -1.5256, -1.5779],\n",
      "          [-1.4036, -1.3513, -1.5430,  ..., -1.4733, -1.5081, -1.5081],\n",
      "          ...,\n",
      "          [-1.7870, -1.7870, -1.7870,  ...,  0.3568,  0.6531,  0.6705],\n",
      "          [-1.7696, -1.7696, -1.8044,  ...,  0.6356,  0.4265,  0.5311],\n",
      "          [-1.7696, -1.7870, -1.8044,  ...,  0.3916,  0.0605,  0.2348]]]])\n",
      "yes      <- there a cat in the picture\n",
      "yes      <- is a cat in the picture\n",
      "yes      <- is there cat in the picture\n",
      "yes      <- is there a in the picture\n",
      "yes      <- is there a cat the picture\n",
      "yes      <- is there a cat in picture\n",
      "yes      <- is there a cat in the\n"
     ]
    }
   ],
   "source": [
    "image_idx = 1\n",
    "\n",
    "leave_one_out(image_files[image_idx], \"is there a cat in the picture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqS5cuZM-oef"
   },
   "source": [
    "#### Leave all combos of words out ( think : Binary )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "pTKGZ-dt-oeg"
   },
   "outputs": [],
   "source": [
    "def leave_out_combos(image_filename, question_base):\n",
    "    plt.imshow(Image.open(image_filename).convert('RGB')); plt.show()    \n",
    "    image_features = resnet_layer4.image_to_features(image_filename)\n",
    "    question_arr = question_base.lower().split(' ')\n",
    "    for i in range(2 ** len(question_arr)):\n",
    "        q_arr = [question_arr[j] for j in range(len(question_arr)) if (i & (2**j))==0 ]\n",
    "        question_str = ' '.join( q_arr )\n",
    "        _, answer_idx = vqa_single_softmax(image_features, question_str).max(dim=1)\n",
    "        print((answer_words[ answer_idx ]+' '*8)[:8]+\" <- \"+question_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "KQtpsgxq-oeh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-2.0323, -2.0152, -2.0152,  ..., -1.6213, -1.6555, -1.7754],\n",
      "          [-2.0323, -2.0665, -2.0494,  ..., -1.6384, -1.6727, -1.7925],\n",
      "          [-1.6555, -1.6384, -1.8268,  ..., -1.6555, -1.6898, -1.7412],\n",
      "          ...,\n",
      "          [-1.9980, -1.9638, -1.9638,  ...,  0.7591,  1.1015,  1.1187],\n",
      "          [-1.9980, -1.9638, -1.9980,  ...,  0.8104,  0.7248,  0.6563],\n",
      "          [-2.0152, -1.9980, -1.9980,  ...,  0.8789,  0.5707,  0.4679]],\n",
      "\n",
      "         [[-1.9657, -1.9307, -1.9307,  ..., -1.5805, -1.6155, -1.7206],\n",
      "          [-1.9657, -1.9832, -1.9657,  ..., -1.5980, -1.6331, -1.7206],\n",
      "          [-1.5805, -1.5455, -1.7381,  ..., -1.5630, -1.5980, -1.6331],\n",
      "          ...,\n",
      "          [-1.9307, -1.9307, -1.9307,  ...,  0.6429,  1.0280,  1.0105],\n",
      "          [-1.9307, -1.9482, -1.9657,  ...,  0.6254,  0.4853,  0.6078],\n",
      "          [-1.9132, -1.9482, -1.9832,  ...,  0.5378,  0.1877,  0.3803]],\n",
      "\n",
      "         [[-1.7696, -1.6999, -1.6999,  ..., -1.4907, -1.5256, -1.5953],\n",
      "          [-1.7522, -1.7522, -1.7347,  ..., -1.4907, -1.5256, -1.5779],\n",
      "          [-1.4036, -1.3513, -1.5430,  ..., -1.4733, -1.5081, -1.5081],\n",
      "          ...,\n",
      "          [-1.7870, -1.7870, -1.7870,  ...,  0.3568,  0.6531,  0.6705],\n",
      "          [-1.7696, -1.7696, -1.8044,  ...,  0.6356,  0.4265,  0.5311],\n",
      "          [-1.7696, -1.7870, -1.8044,  ...,  0.3916,  0.0605,  0.2348]]]])\n",
      "yes      <- is there a cat in the picture\n",
      "yes      <- there a cat in the picture\n",
      "yes      <- is a cat in the picture\n",
      "yes      <- a cat in the picture\n",
      "yes      <- is there cat in the picture\n",
      "yes      <- there cat in the picture\n",
      "yes      <- is cat in the picture\n",
      "yes      <- cat in the picture\n",
      "yes      <- is there a in the picture\n",
      "yes      <- there a in the picture\n",
      "yes      <- is a in the picture\n",
      "yes      <- a in the picture\n",
      "yes      <- is there in the picture\n",
      "yes      <- there in the picture\n",
      "yes      <- is in the picture\n",
      "yes      <- in the picture\n",
      "yes      <- is there a cat the picture\n",
      "yes      <- there a cat the picture\n",
      "no       <- is a cat the picture\n",
      "no       <- a cat the picture\n",
      "yes      <- is there cat the picture\n",
      "yes      <- there cat the picture\n",
      "yes      <- is cat the picture\n",
      "yes      <- cat the picture\n",
      "yes      <- is there a the picture\n",
      "yes      <- there a the picture\n",
      "yes      <- is a the picture\n",
      "yes      <- a the picture\n",
      "yes      <- is there the picture\n",
      "yes      <- there the picture\n",
      "yes      <- is the picture\n",
      "yes      <- the picture\n",
      "yes      <- is there a cat in picture\n",
      "yes      <- there a cat in picture\n",
      "yes      <- is a cat in picture\n",
      "no       <- a cat in picture\n",
      "yes      <- is there cat in picture\n",
      "yes      <- there cat in picture\n",
      "yes      <- is cat in picture\n",
      "no       <- cat in picture\n",
      "yes      <- is there a in picture\n",
      "yes      <- there a in picture\n",
      "yes      <- is a in picture\n",
      "yes      <- a in picture\n",
      "yes      <- is there in picture\n",
      "yes      <- there in picture\n",
      "yes      <- is in picture\n",
      "yes      <- in picture\n",
      "yes      <- is there a cat picture\n",
      "yes      <- there a cat picture\n",
      "no       <- is a cat picture\n",
      "no       <- a cat picture\n",
      "no       <- is there cat picture\n",
      "yes      <- there cat picture\n",
      "yes      <- is cat picture\n",
      "no       <- cat picture\n",
      "yes      <- is there a picture\n",
      "yes      <- there a picture\n",
      "yes      <- is a picture\n",
      "yes      <- a picture\n",
      "yes      <- is there picture\n",
      "yes      <- there picture\n",
      "yes      <- is picture\n",
      "yes      <- picture\n",
      "yes      <- is there a cat in the\n",
      "yes      <- there a cat in the\n",
      "no       <- is a cat in the\n",
      "no       <- a cat in the\n",
      "yes      <- is there cat in the\n",
      "no       <- there cat in the\n",
      "no       <- is cat in the\n",
      "yes      <- cat in the\n",
      "yes      <- is there a in the\n",
      "yes      <- there a in the\n",
      "yes      <- is a in the\n",
      "yes      <- a in the\n",
      "yes      <- is there in the\n",
      "yes      <- there in the\n",
      "yes      <- is in the\n",
      "yes      <- in the\n",
      "yes      <- is there a cat the\n",
      "yes      <- there a cat the\n",
      "no       <- is a cat the\n",
      "no       <- a cat the\n",
      "no       <- is there cat the\n",
      "no       <- there cat the\n",
      "yes      <- is cat the\n",
      "yes      <- cat the\n",
      "yes      <- is there a the\n",
      "yes      <- there a the\n",
      "yes      <- is a the\n",
      "yes      <- a the\n",
      "yes      <- is there the\n",
      "yes      <- there the\n",
      "yes      <- is the\n",
      "yes      <- the\n",
      "yes      <- is there a cat in\n",
      "yes      <- there a cat in\n",
      "no       <- is a cat in\n",
      "no       <- a cat in\n",
      "yes      <- is there cat in\n",
      "yes      <- there cat in\n",
      "yes      <- is cat in\n",
      "no       <- cat in\n",
      "yes      <- is there a in\n",
      "yes      <- there a in\n",
      "yes      <- is a in\n",
      "yes      <- a in\n",
      "yes      <- is there in\n",
      "yes      <- there in\n",
      "no       <- is in\n",
      "no       <- in\n",
      "yes      <- is there a cat\n",
      "yes      <- there a cat\n",
      "yes      <- is a cat\n",
      "yes      <- a cat\n",
      "yes      <- is there cat\n",
      "yes      <- there cat\n",
      "no       <- is cat\n",
      "no       <- cat\n",
      "yes      <- is there a\n",
      "yes      <- there a\n",
      "yes      <- is a\n",
      "yes      <- a\n",
      "yes      <- is there\n",
      "yes      <- there\n",
      "no       <- is\n",
      "yes      <- \n"
     ]
    }
   ],
   "source": [
    "image_idx = 1\n",
    "image_files=['./img/penguins.jpg',\n",
    " './img/2.jpg',\n",
    " './img/ima.jpg',\n",
    " './img/cat_roof_home_architecture_building_roofs_animal_sit-536976.jpg!d']\n",
    "leave_out_combos(image_files[image_idx], \"is there a cat in the picture\")\n",
    "#leave_out_combos(image_files[image_idx], \"what color are cat's eyes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "MfXWnG66-oej"
   },
   "outputs": [],
   "source": [
    "def leave_out_best(image_filename, question_base):\n",
    "    plt.imshow(Image.open(image_filename).convert('RGB')); plt.show()    \n",
    "    image_features = resnet_layer4.image_to_features(image_filename)\n",
    "    _, answer_true = vqa_single_softmax(image_features, question_base).max(dim=1)\n",
    "    print((answer_words[ answer_true ]+' '*8)[:8]+\" <- \"+question_base)\n",
    "    print()\n",
    "    while True:\n",
    "        question_arr = question_base.lower().split(' ')\n",
    "        score_best, q_best = None, ''\n",
    "        for i, word_omit in enumerate(question_arr):\n",
    "            question_str = ' '.join( question_arr[:i]+question_arr[i+1:] )\n",
    "            score, answer_idx = vqa_single_softmax(image_features, question_str).max(dim=1)\n",
    "            if answer_idx==answer_true:\n",
    "                print((answer_words[ answer_idx ]+' '*8)[:8]+\" <- \"+question_str)  #, score        \n",
    "                if (score_best is None or score>score_best):\n",
    "                    score_best, question_base = score, question_str\n",
    "        print()\n",
    "        if score_best is None or len(question_base)==0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "B8dsvcqH-oej",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-2.0323, -2.0152, -2.0152,  ..., -1.6213, -1.6555, -1.7754],\n",
      "          [-2.0323, -2.0665, -2.0494,  ..., -1.6384, -1.6727, -1.7925],\n",
      "          [-1.6555, -1.6384, -1.8268,  ..., -1.6555, -1.6898, -1.7412],\n",
      "          ...,\n",
      "          [-1.9980, -1.9638, -1.9638,  ...,  0.7591,  1.1015,  1.1187],\n",
      "          [-1.9980, -1.9638, -1.9980,  ...,  0.8104,  0.7248,  0.6563],\n",
      "          [-2.0152, -1.9980, -1.9980,  ...,  0.8789,  0.5707,  0.4679]],\n",
      "\n",
      "         [[-1.9657, -1.9307, -1.9307,  ..., -1.5805, -1.6155, -1.7206],\n",
      "          [-1.9657, -1.9832, -1.9657,  ..., -1.5980, -1.6331, -1.7206],\n",
      "          [-1.5805, -1.5455, -1.7381,  ..., -1.5630, -1.5980, -1.6331],\n",
      "          ...,\n",
      "          [-1.9307, -1.9307, -1.9307,  ...,  0.6429,  1.0280,  1.0105],\n",
      "          [-1.9307, -1.9482, -1.9657,  ...,  0.6254,  0.4853,  0.6078],\n",
      "          [-1.9132, -1.9482, -1.9832,  ...,  0.5378,  0.1877,  0.3803]],\n",
      "\n",
      "         [[-1.7696, -1.6999, -1.6999,  ..., -1.4907, -1.5256, -1.5953],\n",
      "          [-1.7522, -1.7522, -1.7347,  ..., -1.4907, -1.5256, -1.5779],\n",
      "          [-1.4036, -1.3513, -1.5430,  ..., -1.4733, -1.5081, -1.5081],\n",
      "          ...,\n",
      "          [-1.7870, -1.7870, -1.7870,  ...,  0.3568,  0.6531,  0.6705],\n",
      "          [-1.7696, -1.7696, -1.8044,  ...,  0.6356,  0.4265,  0.5311],\n",
      "          [-1.7696, -1.7870, -1.8044,  ...,  0.3916,  0.0605,  0.2348]]]])\n",
      "yes      <- is there a cat in the picture\n",
      "\n",
      "yes      <- there a cat in the picture\n",
      "yes      <- is a cat in the picture\n",
      "yes      <- is there cat in the picture\n",
      "yes      <- is there a in the picture\n",
      "yes      <- is there a cat the picture\n",
      "yes      <- is there a cat in picture\n",
      "yes      <- is there a cat in the\n",
      "\n",
      "yes      <- there a in the picture\n",
      "yes      <- is a in the picture\n",
      "yes      <- is there in the picture\n",
      "yes      <- is there a the picture\n",
      "yes      <- is there a in picture\n",
      "yes      <- is there a in the\n",
      "\n",
      "yes      <- a in the picture\n",
      "yes      <- there in the picture\n",
      "yes      <- there a the picture\n",
      "yes      <- there a in picture\n",
      "yes      <- there a in the\n",
      "\n",
      "yes      <- in the picture\n",
      "yes      <- there the picture\n",
      "yes      <- there in picture\n",
      "yes      <- there in the\n",
      "\n",
      "yes      <- the picture\n",
      "yes      <- there picture\n",
      "yes      <- there the\n",
      "\n",
      "yes      <- the\n",
      "yes      <- there\n",
      "\n",
      "yes      <- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_idx = 1\n",
    "\n",
    "leave_out_best(image_files[image_idx], \"is there a cat in the picture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOw0v0GP-oek"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of VQA_playground.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "252a94fb074a4f0798664e217d7dda54": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e48e66d514243db9623f6d29cf19e04": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3618a5c9a96e4317acae6bf1b7c52975": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "87ca56ccc6c3444ba310ac1195074d3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8e750c9d72b044aa86fc395f3f24dc92",
       "IPY_MODEL_e9fe826b5bba48b18a99a5bd67b7381b"
      ],
      "layout": "IPY_MODEL_8ed2ea24126b42a989c7d67be8543668"
     }
    },
    "8e750c9d72b044aa86fc395f3f24dc92": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e48e66d514243db9623f6d29cf19e04",
      "max": 241520640,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3618a5c9a96e4317acae6bf1b7c52975",
      "value": 241520640
     }
    },
    "8ed2ea24126b42a989c7d67be8543668": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0c6e1b729e74b018fbb021cc541aa0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e9fe826b5bba48b18a99a5bd67b7381b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_252a94fb074a4f0798664e217d7dda54",
      "placeholder": "​",
      "style": "IPY_MODEL_a0c6e1b729e74b018fbb021cc541aa0e",
      "value": " 230M/230M [00:09&lt;00:00, 24.5MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}